---
title: Solve Sequence Tasks Using Recurrent Neural Network
date: 2021-03-30
tags: AI, machine learning, sequence model
index_img: /images/rnn/rnn.png
---
### Understand the characteristics of a sequence task 
##### Sequence data
We have previously seen feeding image data into either a general or convolutional nerual network for training, and have input data presented as vectors of image pixels in RGB channels. By learning the distribution of the pixels on the model, the neural network is able to predict and classify the objects in an image. However, there are problems that involve forms of data other than the images, for example, a text script, which is a typial type of sequence data. So, what is a sequence data and how do we define a sequence task? To put it simple, a sequence data, also known as sequential data, is represented as data values over `time`. Comparing to the image where the data values are structured in matrix spaces, sequence data is ordered by timestamps. Some examples of sequence data could be a clip of speech audio where the data is transformed into the magnitudes of the sound wave over time, a text document composed by words and sentences which are ordered by the grammatical manners, or even a video clip that is composed of a sequence of frames. 

##### Sequence tasks 
Sequence tasks refer to the problems which involve sequence data, and they can be classified into different kinds based on the relationship between input data $X$ and output data $Y$. Machine translation is a typical type of `many-to-many` sequence task, meaning a sequnce of input data values $x^{\langle t \rangle}$($t$ here refers to the timestamp, which varies between $1$ and $T_x$. Notation will be further explained in the following section) will be corresponding to a sequence of output data $y^{\langle t \rangle}$. There is also `many-to-one` sequnce task, where sequence data $X$ generates only one prediction value $Y$. Sentiment classification is a good example of having many-to-one mapping between input data and output in which given a comment, the model predict the satisfaction level(can be visualized as a score or a number of stars) of the customer. Furthermore, there is even `one-to-many` mapping for the sequence problem. For example, music generation model takes in a single music note or a word representing the music genres, and outputs a sequence of music notes. 

### Basic recurrent neural network model 
##### Motivation 
Recurrent neural network is particularly used for solving the sequnce tasks. So why don't we just use the standard neural network where the input data gets passed in the hidden layers? Turns out that the characteristics of the sequence data don't work well with the standard model. Unlike the image, the lengths of input and output of sequence data can be different accross different training examples. For example, the training data for training machine translation can be text sentences of different lengths, thus the standard neural network doesn't apply to these training data since the number of the parameters(i.e., the weights and bias) are fixed for each hidden layer. Another important factor that makes standard neural network not suitable for the sequence data is that in the standard NN, the model doesn't share the learned features across different position of a training data. For example, normally "Hawaii" is always a place name regardless of what position the word is in the text. In order to make a standard NN have nice prediction on the word, "Hawaii" needs to appear in different positions across the training examples, which is very inefficient for training the model. 

Furthermore, the size of the parameters also needs to be taken into consideration when training a neural network model. In sequence tasks, the sequence data is normally structured as so called a `one-hot vector` in recurrent neural network(will further explain in the following section). One-hot vector is basically a one-dimension vector with all the cells being $0$ except one cell being $1$ that is used to identify what word the data value is. The size of the one-hot vector could be very large(e.g: $[10,000, 1]$), and that requires a lot of parameters to train if use a standard NN. Therefore, we need a neural network that can gently handle the sequence data input, at the same time, able to share the learned feature across different training examples. And that will be recurrent neural network.

##### Notation and the structure of the RNN model
In a recurrent neural network, the input data $X$ is expressed as [$x^{\langle 1 \rangle}$, $x^{\langle 2 \rangle}$, ..., $x^{\langle T_x \rangle}$] where $T_x$ is the last timestamp of the input sequence, which is also the length of the sequence data. Similarly, the ouptut $Y$ is indicated by the timestamp $t$ as [$y^{\langle 1 \rangle}$, $y^{\langle 2 \rangle}$, ..., $y^{\langle T_y \rangle}$]. The following diagram explains how the output $Y$ is generated for each timestamp. 
![Site Image](/images/rnn/rnn.png)
To output $y^{\langle t \rangle}$, $x^{\langle t \rangle}$ is passed into a RNN cell which is similar to the hidden layer of a standard NN. In the RNN cell, $x^{\langle t \rangle}$ will be computed with the activation function $a^{t-1}$ to get the activation function $a^{t}$ for the current timestamp. Then $a^{t}$ will be used to output $y^{\langle t \rangle}$. 

Notice that the weights for computing the activation function and output $Y$ are usually denoted by two indices as $w_{ab}$, where the first index $a$ refers to the entity that the weight is computing for, and the second index $b$ means the entity the weight is multiplied with. For example, $w_{ax}$ will be the weight parameter for computing the activation function $a$, and the element to multiply with will be the input data $x^{\langle t \rangle}$. The followings are the formulas for computing the activation function and the output at timestamp t:
$a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)$,
$\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)$
Once the activation function $a^{\langle t \rangle}$ is computed, it will be passed to the next RNN cell for computing the hidden state $a^{\langle t + 1 \rangle}$. The above formulas demonstrate the most basic unit of the forward propagation in a RNN, and this computation will carry on across the timestamps until reaches to the end of the sequnce data $x^{\langle T_x \rangle}$. 

Similar to the stand NN, in order to update weight and bias parameters during the backward propagation, there needs to be a loss function for computing the gradient descent. Since RNN predicts output for each timestamp, each output value $\hat{y}^{\langle t \rangle}$ has an associated loss function. All of these individual loss functions will finally be aggregated to a single loss function that is used for computing the gradient descent and propagating backward through time. The followings are the loss functions for the output value at timestamp t and the aggregation respectively:
$\mathcal{L^{\langle t \rangle}}(\hat{y}^{\langle t \rangle}, y^{\langle t \rangle}) =  - y^{\langle t \rangle}  \log(\hat{y}^{\langle t \rangle}) - (1-y^{\langle t \rangle} )  \log(1 - \hat{y}^{\langle t \rangle})$,
$\mathcal{L}(\hat{y}, y) =  \sum_{t=1}^{T_y}\mathcal{L^{\langle t \rangle}}(\hat{y}^{\langle t \rangle}, y^{\langle t \rangle})$

##### Language model with basic RNN
One of the applications of using basic RNN is language model. Langulage model is a system that estimates how "natural" a sentence is. More specifically, the model predicts the probability of the given sentence and determines how likely the sentence can occur in real world. Language model is widely used in machine translation, sequence generation, speech recognition, and so on. One example will be given the beginning texts of the sentence, auto-fill the rest of the sentence. This application can be usually seen in many realtime chat application where the user types out the first few words, and the app will list out the suggestions for the rest of the content. 

The core idea of building a language model is using RNN to estimate the probability of a word, given the conditions of its previous words. The following diagram shows how the language model is trained with a RNN. 
![Site Image](/images/rnn/language-model.jpg)
Assume we have a training example: `The person is walking on the street` for the RNN model. The first step is tokenizing the sentence into `[The, person, is, walking, on, the, street, <EOS>]` where `<EOS>` indicates the end of sentence. These tokenized words will be referred to the output data values [$y^{\langle 1 \rangle}$, $y^{\langle 2 \rangle}$, ..., $y^{\langle 8 \rangle}$]. 

How RNN passes the training data forward over the timestamps is that, the activation function and the data input at the first timestamp will be initialized as zero vectors. The first output value $\hat{y}^{\langle 1 \rangle}$ will be the vector where each entry is the probability of the coresponding word in the pre-defined word dictionary. Then the correct word which is "The" in this case, will be passed to the second timestamp as the input data, meaning $x^{\langle 2 \rangle} = y^{\langle 1 \rangle}$. Note that $y^{\langle 1 \rangle}$ here is a one-hot vector where all the entries are $0$ except the cell of the word "The". Then the prediction value at the second timestamp will be the probability of the word, given the condition that the last word is "The", which can be expressed as $P(\_ | The)$. The same process continues for the rest of the tokens of the training example. 

Overall, the probability of the input word $x^{\langle t \rangle}$ at timestamp t can be generalized as $P(x_t|X_{[0, t-1]})$, where $X_{[i,j]}$ refers to the input data [$x_i$, $x_{i+1}$, ..., $x_{j-1}$, $x_j$]. The probability of the whole sentence $X$ can be expressed as $P(X)=P(x_0)\prod_{t=1}^{T_x}P(x_t|X_{[0,t-1]})$. The higher this probability is, the more likely the given sentence can occur in real life. 

### Capture long-range connections in recurrent neural network 
##### Vanishing gradient 
Basic RNN works fine for simple, short sentences, however, has poor performance on capturing long-range connections. For example, in sentence "That person who is wearing a nice red shirt is walking to me right now.", `is` has connection to `That person`, in which the neural network needs to have a sort of memory cells to store the dependecies between the words that are far apart in the sentence. When sentence gets longer and the words are more connected, the deeper the basic RNN will be and more likely the updates of parameters will vanish during backward propagation. This issue is commonly known as `vanishing gradient` in deep neural network. Vanishing gradient refers to the problem where the neural network gets too deep(i.e., has too many hidden layers), and the gradient of the loss function becomes more hardly to propagate backward through the network and effect the weights and bias in the early layers. In basic RNN, the vanishing gradient will result in the lost connections between the beginning and the ending of the sequence. Therefore, some modifications to the basic RNN are needed in order to eliminate the vanishing gradients. 

##### Gated recurrent unit(GRU)
GRU is a refined version of basic RNN, which has good performance on capturing the long-term dependencies between words within the sequence. The core idea of GRU is to add additional memory cell to the network layer so some features of the input word(e.g: whether the object is singular or plural) can be stored and propagate forward to the dependency words which are far back in the sequence.

A GRU is composed of the following components:
- $c^{\langle t \rangle}$: This is the memory cell of GRU, which stores the binary value(i.e., 0 or 1) for GRU units to memorize and propagate through the sequence. The stored binary value indicates the feature of the data input that wants to be memorized for the later input. For example, a target word needs to be memorized as singular or plural, and the memory cell can store as 0 for singular and 1 for plural. When the dependency word sees the value of the propagated memory cell value, the RNN unit knows whether the target word is singular or plural. 

- $\tilde{c}^{\langle t \rangle}$: This is the candidate value for the memory cell $c^{\langle t \rangle}$. The candidate value is used to overwrite $c^{\langle t \rangle}$ at the timestamp where the memory cell needs to be updated. The equation for updating memory cell at timestamp t is: $\tilde{c}^{\langle t \rangle} = \tanh(W_c[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c)$. Notice that $[c^{\langle t-1 \rangle}, x^{\langle t \rangle}]$ is a concentrated matrix for $c^{\langle t-1 \rangle}$ and $x^{\langle t \rangle}$, and $W_c[c^{\langle t-1 \rangle}, x^{\langle t \rangle}]$ can also be expressed as $W_{cc}c^{\langle t-1 \rangle} + W_{cx}x^{\langle t \rangle}$.

- $\Gamma_u^{\langle t \rangle}$: This is the update gate for the memory cell, which is also the key component of a GRU. The only job of the update gate is to decide whether the memory cell $c^{\langle t \rangle}$ needs to be updated at the timestamp t. For example, in the sentence "That person who is wearing a nice red shirt is walking to me right now", the update gate will update the memory cell to 0 at "That person" since it is the object that needs to be memorized throughout the sequence. The value of the udpate gate is computed using `sigmoid` function, thus the value is between 0 and 1. Intuitively, we'll think of the value of each update gate as either 0 or 1, representing whether the memory cell needs to be updated or not. The equation of computing the update gate is: $\Gamma_o^{\langle t \rangle}=  \sigma(W_u[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)$.

Overall, the equation of updating the memory cell $c^{\langle t \rangle}$ becomes to: $c^{\langle t \rangle} = \Gamma_u^{\langle t \rangle}* \tilde{c}^{\langle t \rangle} + (1-\Gamma_u^{\langle t \rangle}) *{c}^{\langle {t-1} \rangle}$. Notice that when $\Gamma_u^{\langle t \rangle}$ is 0, the value of the memory cell at timestamp t will be just the same as the previous timestamp, and will not be updated. If $\Gamma_u^{\langle t \rangle}$ is 1 instead, then $c^{\langle t \rangle}$ will be updated with the candidate value $\tilde{c}^{\langle t \rangle}$. 

##### Long short-term memory network(LSTM)
LSTM-cell is a refined version of GRU, which is more powerful and robust on capturing the grammatical connections in a text sequence. Instead of just using one update gate to update the memeory cell, LSTM-cell introduces `forget gate` $\Gamma_f^{\langle t \rangle}$ in the equation of updating $c^{\langle t \rangle}$. The forget gate is also computed with the sigmoid function and the value is between 0 and 1, and tells whether the value of the current memory cell should be forgotten. If there is a new subject in the sequence, or the subject is changed from singular to plural, we need to get rid of the previously stored memory cell value and updated with the new candidate value. LSTM-cell gives the memory cell the option to keep the old value $c^{\langle {t-1} \rangle}$ and add the new value onto it. The equation now becomes to: $c^{\langle t \rangle} = \Gamma_f^{\langle t \rangle}* c^{\langle t-1 \rangle} + \Gamma_u^{\langle t \rangle} *\tilde{c}^{\langle t \rangle}$.

### Conclusion 
The RNN models, GRU, and LSTM are all powerful tools to analyze human language and build up language models in NLP, and are also the fundamental units for solving other more complicated sequence tasks such as sentiment classification, machine translation, speech recognition, and so on. Those tasks requires additional techniques and algorithms in RNN training, which are the topics we will look into in the next. 

Hope you found this article useful, and I'll see you next time!




 