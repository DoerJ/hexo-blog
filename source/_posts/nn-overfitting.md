---
title: Reduce Data Overfitting in Neural Network Models
date: 2020-11-23
tags: AI, machine learning, optimization
index_img: /images/thumbnail/nn-overfitting.png
---
### Understand overfitting in neural networks
The nature of the training process for a neural network is to study the data distribution of each training sample, build up the model that well contain the distribution of the data points of the whole training set. When machine "sees" a picture, the trained neural network maps the RGB pixel values of picture onto the model to see how the data points distributed and fit in the model graph. However, there could be many cases where the neural network perform well on the training set, but has poor performance of generalizing the model beyond the training data. This phenomenon is called `data overfitting`, which means, the model fits the exisiting training data too well, therefore can't contain any other different data. The following graph demonstrates how a overfitting model differentiate from a more generalized model.
![Site Image](/images/thumbnail/nn-overfitting.png)
The blue and red data points represent two classes of training data. Imagine this model predicts whether the content of a picture is a dog, and the red points could be the pixel values of a dog picture while the blue points are not. The smooth, black function curve represents a more generalized model on the training set, and the green curve is an overfitting model. The green curve is "noisy" as the model is trying to define a "clean boundary" that separates two classes of training data. Therefore, the overfitting model has high accurancy on predicting this specific set of training data, but doesn't quite apply to other general testing data. 

An overfitting model has low bias and high variance. The bias of a model indicates the average difference between the prediction value and the expected value, while the variance measures the amount that the estimation value will change the model is fed with data that comes from a different source. Bias and variance are strongly related to each other in learning process of neural network, and in most of time, the bias-variance trade-off is inevitable. The ideal neural network model has low bias and low variance, however, that is hard to achieve. Instead, we should aim for a "balance point" where the model has both acceptable bias and variance. When start training the model, the first thing we should do is to identify how the performance is on the training data, that is, check whether the model has high or low bias. If the model has high bias, then we should make the network bigger by adding more hidden units. If the bias is in acceptable range, we should then observe whether the model has high variance. To reduce the variance, we can feed the neural network with more training data to make the model to be more adaptive to the general data distribution. However, getting sufficient amount of training data could be expensive in terms of the time spent on searching for suitable data sources and collecting data. Therefore, `regularization` has been one the most common approach to reduce the variance of the model.

### L2 Regularization
Regularization is a technique that discourages of using a very deep, complex neural network. Consider a linear model to have high bias and low variance since the model linearly classify the data and can be generalize across the data pool. Oppositely, a non-linear model which can be construted by a deep and complex neural network usually has low bias and high variance, and this is the issue we intend to solve. The idea of regularization is to transform a deep neural network into a simpler network such as a logistic regression model. To achieve that, regularization adds an additional term in the cost function as the following:

$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost}$$

where $\sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2}$ is equivalent to $\sum\limits_l W^{[l]T}.W^{[l]} = \sum\limits_l {\| W^{[l]}\|_2}^2$. Note that the subscript $l$ refers to the index of hidden layer, so $W^{[l]}$ will be the weights of layer $l$. $m$ is the number of training examples, and ${\lambda}$ is a hyperparameter. 

This type of regularization is called L2 regularization, since the added term to the cost function is a `two-norm` of the weights. Like what we mentioned before, the intuition of $L2$ regularization is to transform a deep neural network into a simpler network. If we make the hyperparameter ${\lambda}$ big, in order to minimize the value of the cost function, the weights in two-norm are enforced to be reduced. We can think of it as forcing more punishment to the weights if there is cost on prediction. Therefore, the regularization weakens the effect that the weight has on each hidden unit of each hidden layer, similar to zeroing out the hidden units as the following:
![Site Image](/images/nn-overfitting/regularization.png)
Since the cost function has changed, the backward propagation has to be updated as well. The gradient for the weights has to be computed with respect to the new regularization term. Therefore, the gradient update for $W$ has become $W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} - \alpha\text{ } \frac{\lambda}{m} W^{[l]}$.

### Dropout regularization 
Dropout shares some similarities with L2 regualrization. The core idea of dropout is also minifies the deep network in some degrees so the model becomes less sensitive to input features. However, instead of making the weights of each hidden layer smaller, dropout regularization randomly shuts down some of the hidden units in each layer. Specifically, dropout technique introduces a new hyperparameter `keep_prob` in learning algorithm. $keep\_prob$ indicates the probability that the portion of the nurons will stay in the layer. For example, $keep\_prob^{[l]}=0.8$ means the random 20% of neurons in layer $l$ will be shut down in current iteration. Here is a graph that illustartes the working process of dropout:
![Site Image](/images/nn-overfitting/dropout.png)
Furthermore, since the neurons are randomly shut down, the model to be trained in each iteration will also be different. And that makes the neurons in the model less sensitive to the activation of a specific neuron, therefore, the model becomes smoother because pays less attention to the specific distribution of the training data. 

### Conclusion 
There are also other approaches that solve the overfitting from different perspectives, such as `data augmentation`, which is a technique that takes the existing training data and increase the size of dataset by rotating, fliping, and zooming each training image. Data augmentation is considered as a workaround to gain more training data to decrease the variance. 

Bias and variance of the model are the two of the most important indices we should focus on during the model training, and they have direct impact on the accurancy of the predictions in real-world application. Although regularization takes out some performances on the training data, however, the ultimate goal here is to balnce out the bias-variance trade-off so the model fits the general distribution of the overall training data and has good performance on the testing data at the same time. 

I hope you found this article useful, and stay tuned for more of my thoughts on machine learning!



